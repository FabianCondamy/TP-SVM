---
title: "Compte rendu du TP SVM"
format: html
author: "Fabian Condamy et Samy M'Rad"
jupyter: python3
---

**Question 1**

```{python}
import sys
sys.path.append("scripts_python") # pour aller chercher la fonction de svm_source
import numpy as np
from sklearn import datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from svm_source import plot_2d

scaler = StandardScaler()

iris = datasets.load_iris()
X = iris.data
X = scaler.fit_transform(X)
y = iris.target
X = X[y != 0, :2]
y = y[y != 0]

# split train test (say 25% for the test)
# You can shuffle and then separate or you can just use train_test_split 
#whithout shuffling (in that case fix the random state (say to 42) for reproductibility)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42, stratify=y
)

###############################################################################
# fit the model with linear vs polynomial kernel
svm_linear = SVC(kernel='linear')
svm_linear.fit(X_train, y_train)
y_pred_linear = svm_linear.predict(X_test)
score_linear = svm_linear.score(X_test, y_test)
print('Score du modèle linéaire : %s' % score_linear)

# Plot
plot_2d(X,y)
```

On obtient ici un taux de réussite de 76% avec un nombre de bonnes prédictions de 38 (sur les 50 données au total).


**Question 2**

```{python}
## Code quasiment identique
import numpy as np
from sklearn import datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split

iris = datasets.load_iris()
X = iris.data
y = iris.target

X = X[y != 0, :2]
y = y[y != 0]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.5, random_state=42, stratify=y
)

# Modèle SVM à noyau polynomial de sklearn
svm = SVC(kernel='poly')
svm.fit(X_train, y_train)

y_pred = svm.predict(X_test)

correct = np.sum(y_pred == y_test)
total = len(y_test)
accuracy = correct / total

print(f"Nombre de bonnes prédictions : {correct}/{total}")
print(f"Taux de réussite : {accuracy * 100:.2f}%")
```

Ici, on obtient un taux de réussite de 68%, donc légérèment plus faible qu'auparavant.

**Question 3 (facultative)**

Commençons par générer le jeu de données très déséquilibré :


```{python}
import sys
sys.path.append("scripts_python") # pour aller chercher la fonction de svm_source

import numpy as np
from sklearn.svm import SVC
import matplotlib.pyplot as plt
from svm_source import *
# Création du jeu de données

n = 1000 # nombre d'observations
classes = np.random.choice([0,1], size=n, p=[0.9,0.1]) # classes 1 et 2 avec respectivement p=90% et p=10%

# Affichage pour vérifier
print(np.sort(classes))

# Variables pour SVM
X = np.random.randn(n, 2)
Y = classes

def plot_svm(C_value):
    clf = SVC(kernel='linear', C=C_value)
    clf.fit(X, Y)

    plt.figure()
    plt.title(f"SVM linéaire pour C={C_value}")

    # Tracer les points
    plt.scatter(X[Y == 0][:, 0], X[Y == 0][:, 1], label="Classe 1 (majoritaire)", alpha=0.5)
    plt.scatter(X[Y == 1][:, 0], X[Y == 1][:, 1], label="Classe 2 (minoritaire)", alpha=0.8)

    # Tracer la frontière
    ax = plt.gca()
    xlim = ax.get_xlim()
    ylim = ax.get_ylim()

    xx, yy = np.meshgrid(
        np.linspace(xlim[0], xlim[1], 100),
        np.linspace(ylim[0], ylim[1], 100)
    )
    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    ax.contour(xx, yy, Z, levels=[0], linewidths=2, linestyles="--", alpha=0.7)
    plt.legend()
    plt.show()

# Afficher plusieurs valeurs de C
for C in [1, 0.1, 0.01]:
    plot_svm(C)
```

Sur un jeu de données purement aléatoire, on observe que le paramètre C n'a aucune influence, les frontières n'apparaissent même pas. Ceci est logique car les données sont impossibles à séparer facilement, il n'y a aucune logique sous-jacente à cette répartition des points.

Si on prend maintenant un jeu de données plus structuré :

```{python}
import numpy as np
from sklearn.svm import SVC
import matplotlib.pyplot as plt

# Génération d'un dataset structuré (mais déséquilibré)
n_majoritaire = 90
n_minoritaire = 10

# Classe majoritaire centrée en (0,0)
X_majoritaire = np.random.randn(n_majoritaire, 2) * 0.8 + np.array([0, 0])
# Classe minoritaire centrée en (3,3)
X_minoritaire = np.random.randn(n_minoritaire, 2) * 0.8 + np.array([3, 3])

X = np.vstack((X_majoritaire, X_minoritaire))
Y = np.array([0]*n_majoritaire + [1]*n_minoritaire)

def plot_svm(C_value):
    clf = SVC(kernel='linear', C=C_value)
    clf.fit(X, Y)

    plt.figure()
    plt.title(f"SVM linéaire pour C={C_value}")

    # Tracer les points
    plt.scatter(X[Y == 0][:, 0], X[Y == 0][:, 1], label="Classe 1 (majoritaire)", alpha=0.5)
    plt.scatter(X[Y == 1][:, 0], X[Y == 1][:, 1], label="Classe 2 (minoritaire)", alpha=0.8)

    # Tracer la frontière
    ax = plt.gca()
    xlim = ax.get_xlim()
    ylim = ax.get_ylim()

    xx, yy = np.meshgrid(
        np.linspace(xlim[0], xlim[1], 100),
        np.linspace(ylim[0], ylim[1], 100)
    )
    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    ax.contour(xx, yy, Z, levels=[0], linewidths=2, linestyles="--", alpha=0.7)
    plt.legend()
    plt.show()

# Afficher plusieurs valeurs de C
for C in [1, 0.1, 0.01]:
    plot_svm(C)
```

Ici, on voit l'action du paramètre C, la frontière se déplace dans les différents exemples. 


**Question 4**

```{python}
import sys
sys.path.append("scripts_python") # pour aller chercher la fonction de svm_source

import matplotlib.pyplot as plt
from sklearn.datasets import fetch_lfw_people
import numpy as np
from svm_source import plot_gallery

lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4,
                              color=True, funneled=False, slice_=None,
                              download_if_missing=True)
# introspect the images arrays to find the shapes (for plotting)
images = lfw_people.images
n_samples, h, w, n_colors = images.shape

# the label to predict is the id of the person
target_names = lfw_people.target_names.tolist()

####################################################################
# Pick a pair to classify such as
names = ['Tony Blair', 'Colin Powell']
# names = ['Donald Rumsfeld', 'Colin Powell']

idx0 = (lfw_people.target == target_names.index(names[0]))
idx1 = (lfw_people.target == target_names.index(names[1]))
images = np.r_[images[idx0], images[idx1]]
n_samples = images.shape[0]
y = np.r_[np.zeros(np.sum(idx0)), np.ones(np.sum(idx1))].astype(int)

# plot a sample set of the data
plot_gallery(images, np.arange(12))
plt.show()
```
