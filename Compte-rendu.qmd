---
title: "Compte rendu du TP SVM"
format: html
author: "Fabian Condamy et Samy M'Rad"
jupyter: python3
---

**Question 1**

```{python}
import numpy as np
from sklearn import datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split

# Chargement du jeu de données
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Sélection des bonnes variables
X = X[y != 0, :2]
y = y[y != 0]

# Séparation de la moitié des données
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.5, random_state=42, stratify=y
)

# Modèle SVM à noyau linéaire de sklearn
svm = SVC(kernel='linear')
svm.fit(X_train, y_train)

# Prédiction réalisée sur la partie test après train
y_pred = svm.predict(X_test)

# Évaluation
correct = np.sum(y_pred == y_test)
total = len(y_test)
accuracy = correct / total

print(f"Nombre de bonnes prédictions : {correct}/{total}")
print(f"Taux de réussite : {accuracy * 100:.2f}%")
```

On obtient ici un taux de réussite de 76% avec un nombre de bonnes prédictions de 38 (sur les 50 données au total).


**Question 2**

```{python}
## Code quasiment identique
import numpy as np
from sklearn import datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split

iris = datasets.load_iris()
X = iris.data
y = iris.target

X = X[y != 0, :2]
y = y[y != 0]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.5, random_state=42, stratify=y
)

# Modèle SVM à noyau polynomial de sklearn
svm = SVC(kernel='poly')
svm.fit(X_train, y_train)

y_pred = svm.predict(X_test)

correct = np.sum(y_pred == y_test)
total = len(y_test)
accuracy = correct / total

print(f"Nombre de bonnes prédictions : {correct}/{total}")
print(f"Taux de réussite : {accuracy * 100:.2f}%")
```

Ici, on obtient un taux de réussite de 68%, donc légérèment plus faible qu'auparavant.

**Question 3 (facultative)**

Commençons par générer le jeu de données très déséquilibré :


```{python}
import numpy as np
from sklearn.svm import SVC
import matplotlib.pyplot as plt

# Création du jeu de données

n = 1000 # nombre d'observations
classes = np.random.choice([0,1], size=n, p=[0.9,0.1]) # classes 1 et 2 avec respectivement p=90% et p=10%

# Affichage pour vérifier
print(np.sort(classes))

# Variables pour SVM
X = np.random.randn(n, 2)
Y = classes

def plot_svm(C_value):
    clf = SVC(kernel='linear', C=C_value)
    clf.fit(X, Y)

    plt.figure()
    plt.title(f"SVM linéaire pour C={C_value}")

    # Tracer les points
    plt.scatter(X[Y == 0][:, 0], X[Y == 0][:, 1], label="Classe 1 (majoritaire)", alpha=0.5)
    plt.scatter(X[Y == 1][:, 0], X[Y == 1][:, 1], label="Classe 2 (minoritaire)", alpha=0.8)

    # Tracer la frontière
    ax = plt.gca()
    xlim = ax.get_xlim()
    ylim = ax.get_ylim()

    xx, yy = np.meshgrid(
        np.linspace(xlim[0], xlim[1], 100),
        np.linspace(ylim[0], ylim[1], 100)
    )
    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    ax.contour(xx, yy, Z, levels=[0], linewidths=2, linestyles="--", alpha=0.7)
    plt.legend()
    plt.show()

# Afficher plusieurs valeurs de C
for C in [1, 0.1, 0.01]:
    plot_svm(C)
```

Sur un jeu de données purement aléatoire, on observe que le paramètre C n'a aucune influence, les frontières n'apparaissent même pas. Ceci est logique car les données sont impossibles à séparer facilement, il n'y a aucune logique sous-jacente à cette répartition des points.

Si on prend maintenant un jeu de données plus structuré :

```{python}
# Génération d'un dataset structuré (mais déséquilibré)
n_majoritaire = 90
n_minoritaire = 10

# Classe majoritaire centrée en (0,0)
X_majoritaire = np.random.randn(n_majoritaire, 2) * 0.8 + np.array([0, 0])
# Classe minoritaire centrée en (3,3)
X_minoritaire = np.random.randn(n_minoritaire, 2) * 0.8 + np.array([3, 3])

X = np.vstack((X_majoritaire, X_minoritaire))
Y = np.array([0]*n_majoritaire + [1]*n_minoritaire)

def plot_svm(C_value):
    clf = SVC(kernel='linear', C=C_value)
    clf.fit(X, Y)

    plt.figure()
    plt.title(f"SVM linéaire pour C={C_value}")

    # Tracer les points
    plt.scatter(X[Y == 0][:, 0], X[Y == 0][:, 1], label="Classe 1 (majoritaire)", alpha=0.5)
    plt.scatter(X[Y == 1][:, 0], X[Y == 1][:, 1], label="Classe 2 (minoritaire)", alpha=0.8)

    # Tracer la frontière
    ax = plt.gca()
    xlim = ax.get_xlim()
    ylim = ax.get_ylim()

    xx, yy = np.meshgrid(
        np.linspace(xlim[0], xlim[1], 100),
        np.linspace(ylim[0], ylim[1], 100)
    )
    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    ax.contour(xx, yy, Z, levels=[0], linewidths=2, linestyles="--", alpha=0.7)
    plt.legend()
    plt.show()

# Afficher plusieurs valeurs de C
for C in [1, 0.1, 0.01]:
    plot_svm(C)
```

Ici, on voit l'action du paramètre C, la frontière se déplace dans les différents exemples. 